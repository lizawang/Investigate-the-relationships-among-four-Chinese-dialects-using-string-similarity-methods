# [Needleman-Wunsh Alignment](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm)
- Get the max (similarity) score (higher the match, higher the score)
## Match/mismatch scoring
- Cosine similarity is applied upon the characters in the strings/sentences using (300,) context vectors of the characters that are generated by the pretrained Mandarin character vectors from [fastText-0.9.2 pybind11-2.10.3](https://fasttext.cc/docs/en/crawl-vectors.html) (which is trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives).
- For the characters that are not in the pretrained model, it will return a vector of all 0 which causes "divide by 0 error" when computing the cosine simialrity. To avoid this, a value that doesn't stand out is chosen (0.22, the lowest mean character cosine similarity of all language pairs is employed). (For this reason, 𠲎 is changed into 伐 in both Shanghainese and Wenzhounese because the former character doesn't exist in modern Mandarin (neither in the fasttext model). <br>
The unknown characters for the model in each dialect:
  ```
  wen: {'狃', '剫', '埏', '悗', '眙'},
  wuu: {'砬', '墶', '搿', '隑', '哴', '龌', '㑚', '龊'}
  yue: {'躝', '掹', '𢱕', '埲'}
  ```
- The benefit of using cosine similarity instead of using just fixed values for matching and mismatching is that it provides a value for every character pair based on how similar their context vector is. So that 你 ('you' in Mandarin) and 侬 ('you' in Shanghainese) get a higher matching score than 你 ('you' in Mandarin) and 伐 (genetive in Shanghainese).
## Gap penalty
- It is set to 0 to make sure that the alignment is fully imposed by the characters' cosine similarity values (-1 to 1). <br>
- In this way, the two strings align along the characters that have highest cosine similarities (often 1.0, from the identical characters that two sentences share). 

```
Sentence Alignment:  124
不要玩钥匙-----
---钥匙孛相勿来个

Sentence Alignment:  109
随时问任何问题都可以-
-有---问题就问好唻
```
- The rest is also aligned by cosine similarity value from large to small recursivly. <br>
Why extra "-"? Why not just "为什么-" with "啥个道理" or "-为什么" with "啥个道理"? Because "啥" has the higher similairty score with "什" than "为". The same reason for the second example because "辰" has a much higher similairty score with "候" than "时".
```
Sentence Alignment:  19
她昨天为什么--不来
伊昨日-啥个道理勿来

Sentence Alignment:  91
我年轻的时候-常常去打棒球
我年轻个-辰光常常去打棒球
```
## Final alignment similarity score for each sentence pair
- Each sentence alignment similarity score is normalized by the aligned sentence length (after gap symbol '-' is added).
- The values are between -1 (theoritically when all character pairs have cosine similarity -1) and 1 (when the shorter sentence is aligned with the longer sentence with all identical characters or two sentences are totally identical.)
- But the results show that even the totally different two sentences (even with different word order), the normalized similarity score is above 0. The examples below are from cmn and wuu.

```
Sentence Alignment:  1
--那又怎样
哏末哪能呢-
score: 0.2766460080941518 

Sentence Alignment:  106
走开
滚蛋
score: 0.21459350734949112

Sentence Alignment:  13
参加的-人并不多
呒没多少人-垃海
score: 0.28031292650848627 
```
- The length difference of the two sentences has big impact on the final similarity score (because it is got by dividing the aligned length which is at least the length of the longer sentence/string).

```
Sentence Alignment:  119
安静----
-静一点好伐
score: 0.16666666666666666
```
## Data preprocessing
- Data are Mandarin (cmn) with three dialects: Shanghainese(wuu), Wenzhounese(wen) and Cantonese(yue).
- The following punctuation are removed.

```
 punc = ["。", "，", "！", "？", "'", "\"", ",", ".", "、", "?", "「", "」"]
```
## 1000 times Sampling with replacement
- Each language pairs' sentence alignment similairty scores are fully (the length of all sentence pairs) sampled 1000 times with replacement. 1000 averaged (across all sentence pairs) aligned similarity scores of each language pair is collected and plotted with continuous probability density function from [seaborn](https://seaborn.pydata.org/generated/seaborn.kdeplot.html).

## Result
- wen and wuu are closer than yue and cmn but not as close as their distance to cmn.
- The distances of between yue and wen, and yue and wuu are longest with wen slightly closer to yue.
------------------------------------------------------------------------------------
# Another attempt
Based on the alignment sentence pairs generated by the above method, attempt was made to get all ngram (1-3) substring pairs (e.g. ("你", "侬")) of two languages and count their occurrences globally across all sentence pairs of the two languages. With the ngrams and their counts, try to transform one sentence of a language to the corresponding one in another language by replacing paired ngrams that occur more than once. The result is not very succussful due to the small dataset which can't provide many bigram or trigram phrase pairs that occur more than once. However, after 1000 times sampling with replacement, the distribution shows similar result as the one of cosine similarity, but only that in this case, wuu and wen needs more ngram replacements on average than cmn and yue which indicates wuu and wen are more distant.
